{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statment\n",
    "Santander wants to Identify which customers will make at least one transaction in the future irrespective of the amount. Santander therefore wants a model that will predict future customer transactions with real data provided but unidentified for security purposes.\n",
    "The data has 200 numerical values of anonimyzed data which will be used to analyse the customer relations with the business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning\n",
    "### Exploratory Data Analysis\n",
    "### Data Visualization using Statistical methods\n",
    "### Implementation of most suitable machine learning algorithm\n",
    "### Predict and display future comsumer transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING NECESSARY PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy.stats.stats import pearsonr\n",
    "import itertools\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link to data set from Kaggle\n",
    "#\n",
    "# https://www.kaggle.com/c/santander-customer-transaction-prediction/data\n",
    "\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>0</td>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n",
       "0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n",
       "1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n",
       "2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n",
       "3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n",
       "4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n",
       "\n",
       "     var_7  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  18.6266  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  16.5338  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  14.6155  ...   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  14.9250  ...   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 202 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A DESCRIPTION OF THE DATA SET\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CALCULATE NUMBER OF MISSING VALUES IN TRAIN AND TEST DATAFRAMES\n",
    "def check_null_values(df):\n",
    "    flag=df.isna().sum().any()\n",
    "    if flag==True:\n",
    "        total = df.isnull().sum()\n",
    "        percent = (df.isnull().sum())/(df.isnull().count()*100)\n",
    "        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "        data_type = []\n",
    "        for col in df.columns:\n",
    "            dtype = str(df[col].dtype)\n",
    "            data_type.append(dtype)\n",
    "        output['Types'] = data_type\n",
    "        return(np.transpose(output))\n",
    "    else:\n",
    "        return(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_null_values(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISTRIBUTION OF THE TARGET VALUE IN TRAIN DATASET\n",
    "f,ax=plt.subplots(1,2,figsize=(18,8))\n",
    "data['target'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\n",
    "ax[0].set_title('target')\n",
    "ax[0].set_ylabel('')\n",
    "sns.countplot('target',data=data,ax=ax[1])\n",
    "ax[1].set_title('Target')\n",
    "plt.show()\n",
    "#SINCE THERE ARE 10.0% VALUES WITH 1, THE DATA IS UNBALANCED WRT TARGET VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DENSITY PLOTS OF EACH FEATURE WITH THE TARGET VALUES(0,1) i.e \n",
    "# How each feature infleunces the target being a 0 or a 1 using a density graph.\n",
    "\n",
    "def plot_feature_distribution(df1, df2, label1, label2, features):\n",
    "    i = 0\n",
    "    sns.set_style('whitegrid')\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(10,10,figsize=(18,22))\n",
    "\n",
    "    for feature in features:\n",
    "        i += 1\n",
    "        plt.subplot(10,10,i)\n",
    "        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n",
    "        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n",
    "        plt.xlabel(feature, fontsize=9)\n",
    "        locs, labels = plt.xticks()\n",
    "        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n",
    "        plt.tick_params(axis='y', which='major', labelsize=6)\n",
    "    plt.tight_layout()\n",
    "    plt.show();\n",
    "    \n",
    "t0 = data.loc[data['target'] == 0]\n",
    "t1 = data.loc[data['target'] == 1]\n",
    "features_1 = data.columns.values[2:102]\n",
    "plot_feature_distribution(t0, t1, '0', '1', features_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The last 100 features\n",
    "features_2 = data.columns.values[102:202]\n",
    "plot_feature_distribution(t0, t1, '0', '1', features_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the above Density plots and their infleunce to the target values\n",
    "- Firstly, since the data contained in the train and test dataframes are similar, analysis was done on the test which applies on the train dataframe as well.\n",
    "- From these density plot, most of the features have very high standard deviation values which implies the data points are spread out around the mean, i.e very low minimum values and very high maximum values.\n",
    "- Approximately 5% of these features have very low standard deviations which makes the density plot have peak values at a high probabilistic level of 0.6 which is a great probability of having a customer do a transaction in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson Correlation \n",
    "* From the 200 features, var_91, var_108 and var_148 have the highest probability values from their density plots.\n",
    "* Applying a pearson correlation on each feature and the target to see their relationship\n",
    "* considering var_91\n",
    "* Null hypothesis is: there is a relationship between them\n",
    "* The alternative hypothesis is there is no relationship between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.pearsonr(data['target'], data['var_91'])\n",
    "# since r value is < 0.02, the correlation between target and var_91 is a positive weak correlation \n",
    "# this implies as var_91 increases, probability of having a customer make future transaction increases too\n",
    "# Also since the p value is greater than 0.05, the null hypothesis is therefore supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering var_108\n",
    "stats.pearsonr(data['target'], data['var_108'])\n",
    "# The rvalue being <0.02 and negative shows that there is a weak negative correlation between these variables\n",
    "# meaning an increase in var_108 causes a decrease in target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# considering var_148\n",
    "stats.pearsonr(data['target'], data['var_148'])\n",
    "# The rvalue being <0.02 and negative shows that there is a weak negative correlation between these variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pearson correlation analysis\n",
    "* Var_91 has a positive weak correlation with the target, this implies higher values of var_91 will increase the probability of having a 1.\n",
    "* var_108 and var_148 both have negative weak correlation with the target. This implies a decrease in any of these values causes a increased probability of having 1.\n",
    "* This observation gotten from the pearson correlation of var_91 and target can further be used for implementation as an increase in var_91 increases the probability of a customer making a transaction in the future. \n",
    "* Measure put to increase this unidentified feature could be a great milestone to the project's aim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and test datasets.\n",
    "* Two models are used so the one that produces highest accuracy is therefore termed most suitable for the problem. They are;\n",
    "* Logistic Regression Model with and without regularization\n",
    "* Multinomial Naive Bayes Model \n",
    "* Conclussions are therefore drawn from the accuracies of  the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# since the dataset is too heavy, training and testing of the model will be done on 20000 records\n",
    "#Before splitting the dataset, verify if X is a matrix and y a vector\n",
    "data = data[]\n",
    "X = data.iloc[:, 2:202].values\n",
    "print(\"X: \", type(X), X.shape)\n",
    "y = data.iloc[:, 1].values\n",
    "print('y: ', type(y), y.shape)\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xlr, Xtestlr, ylr, ytestlr = train_test_split(data.iloc[:, 2:202].values, \n",
    "                                              data.iloc[:, 1].values,random_state=5)\n",
    "\n",
    "print('Accuracy of the Logistic Regression model on the training and test set')\n",
    "clf = LogisticRegression()\n",
    "# Fit the model on the trainng data.\n",
    "clf.fit(Xlr, ylr)\n",
    "#Accuracy from the training data\n",
    "y_predict_train = clf.predict(Xlr)\n",
    "print(\"[Train] Accuracy score (ylr, y_predict_train):\", accuracy_score(ylr, y_predict_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy from the testing data.\n",
    "y_predict_test = clf.predict(Xtestlr)\n",
    "print(\"[Test] Accuracy score (y_predict_test, ytestlr):\", accuracy_score(y_predict_test, ytestlr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Since the difference between the training and test accuracy scores is not much, this implies there is no model overfitting or underfitting.\n",
    "* The model is also suitable for this problem as it's accuracy is greater than 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out classification report\n",
    "# Precision: Ability of a classiifer not to label an instance positive that is actually negative. \n",
    "# Recall: Ability of a classifier to find all positive instances for each class it is defined\n",
    "# F1 score: Weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0.\n",
    "#Support: Number of actual occurrences of the class in the specified dataset\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[Training Classification Report:]\")\n",
    "print(classification_report(ylr, y_predict_training))\n",
    "\n",
    "print(\"[Test Classification Report:]\")\n",
    "print(classification_report(ytestlr, y_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def cv_score(clf, x, y, score_func=accuracy_score):\n",
    "    result = 0\n",
    "    nfold = 2\n",
    "    for train, test in KFold(nfold).split(x): # split data into train/test groups, 2 times\n",
    "        clf.fit(x[train], y[train]) # fit\n",
    "        result += score_func(clf.predict(x[test]), y[test]) # evaluate score function on held-out data\n",
    "    return result / nfold # average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGR with l1 regularization\n",
    "clf_1 = LogisticRegression(penalty='l1', random_state = 0)\n",
    "score = cv_score(clf_1, Xlr, ylr)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Multinomial Naive Bayes Model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X = data.iloc[:, 2:202].values\n",
    "print(\"X: \", type(X), X.shape)\n",
    "y = data.iloc[:, 1].values\n",
    "print('y: ', type(y), y.shape)\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(data.iloc[:, 2:202].values, \n",
    "                                              data.iloc[:, 1].values,random_state=5)\n",
    "\n",
    "clf_mnb = MultinomialNB()\n",
    "# Fit the model on the trainng data.\n",
    "clf_mnb.fit(Xtrain, ytrain)\n",
    "#Accuracy from the training data\n",
    "y_predict_train = clf_mnb.predict(X)\n",
    "print('Accuracy of the Multinomial Naive Bayes model on the training and test set')\n",
    "print(\"[Train] Accuracy score (ylr, y_predict_train):\", accuracy_score(ylr, y_predict_train))\n",
    "\n",
    "# Print the accuracy from the testing data.\n",
    "y_predict_test = clf_mnb.predict(Xtestlr)\n",
    "print(\"[Test] Accuracy score (y_predict_test, ytestlr):\", accuracy_score(y_predict_test, ytestlr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
